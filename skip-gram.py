# -*- coding: utf-8 -*-
"""skip-gram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CYPF5g24FACfcblK2sLmly_qeJQt-iSP
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import string
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

df = pd.read_csv('train.csv').head(20000)

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    tokens = [word.translate(str.maketrans('', '', string.punctuation)) for word in tokens]
    return tokens

df['Description'] = df['Description'].apply(preprocess_text)

word_counts = Counter()
for description in df['Description']:
    word_counts.update(description)

vocab = {word: index for index, (word, _) in enumerate(word_counts.most_common())}
idx_to_word = {index: word for word, index in vocab.items()}

def generate_samples(description, window_size, num_negative_samples, vocab):
    samples = []
    for i, target_word in enumerate(description):
        context_words = description[max(0, i - window_size):i] + description[i+1:i+window_size+1]
        for context_word in context_words:
            samples.append((vocab[target_word], vocab[context_word], 1))
            for _ in range(num_negative_samples):
                negative_word = np.random.choice(list(vocab.values()))
                while negative_word == vocab[context_word]:
                    negative_word = np.random.choice(list(vocab.values()))
                samples.append((vocab[target_word], negative_word, 0))
    return samples

class WordEmbeddingDataset(Dataset):
    def __init__(self, descriptions, window_size, num_negative_samples, vocab):
        self.samples = []
        for description in descriptions:
            self.samples.extend(generate_samples(description, window_size, num_negative_samples, vocab))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

class SkipGramNegSampling(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramNegSampling, self).__init__()
        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.init_emb()

    def init_emb(self):
        init_range = 0.5 / self.target_embeddings.embedding_dim
        self.target_embeddings.weight.data.uniform_(-init_range, init_range)
        self.context_embeddings.weight.data.uniform_(-0, 0)

    def forward(self, target_words, context_words):
        target_emb = self.target_embeddings(target_words)
        context_emb = self.context_embeddings(context_words)
        scores = torch.matmul(target_emb, context_emb.transpose(0, 1))
        return scores

window_size = 4
num_negative_samples = 5
embedding_dim = 300
learning_rate = 0.001
batch_size = 1024
num_epochs = 5

dataset = WordEmbeddingDataset(df['Description'], window_size, num_negative_samples, vocab)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

model = SkipGramNegSampling(len(vocab), embedding_dim).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    total_loss = 0
    for i, (target_words, context_words, labels) in enumerate(dataloader):
        target_words, context_words, labels = target_words.to(device), context_words.to(device), labels.to(device)
        optimizer.zero_grad()
        scores = model(target_words, context_words)
        loss = criterion(scores.view(-1), labels.float())
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        if (i+1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {total_loss/(i+1):.4f}')

word_embeddings = model.target_embeddings.weight.detach().cpu().numpy()

torch.save({'word_vectors': torch.tensor(word_embeddings), 'vocab': vocab}, 'skip-gram-word-vectors.pt')