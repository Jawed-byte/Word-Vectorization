# -*- coding: utf-8 -*-
"""skip-gram-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OBYJXsxqYNfstn5y4G7Gp7v3d-MQuQFL
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import string
from collections import Counter
from tqdm import tqdm
import torch.nn.utils.rnn as rnn_utils
from sklearn.metrics import precision_recall_fscore_support

class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        out = self.fc(h_n[-1])
        return out

class CustomDataset(Dataset):
    def __init__(self, data, word_vectors, vocab, max_len):
        self.data = data
        self.word_vectors = word_vectors
        self.vocab = vocab
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        description = self.data.iloc[idx]['Description']
        label = self.data.iloc[idx]['Class Index'] - 1
        tokenized_desc = self.preprocess_text(description)
        desc_vector = self.get_sentence_vector(tokenized_desc)
        return desc_vector, label

    def preprocess_text(self, text):
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
        return text.split()

    def get_sentence_vector(self, tokens):
        vector = torch.zeros(self.max_len, self.word_vectors.shape[1], device=device)
        for i, token in enumerate(tokens):
            if i >= self.max_len:
                break
            if token in self.vocab:
                vector[i] = torch.tensor(self.word_vectors[self.vocab[token]], device=device)
        return rnn_utils.pad_sequence([vector], batch_first=True).squeeze(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

svd_data = torch.load('skippy-skippy4.pt', map_location=device)
word_vectors = svd_data['word_vectors']
vocab = svd_data['vocab']

train_df = pd.read_csv('train.csv').head(20000)
test_df = pd.read_csv('test.csv')

input_size = word_vectors.shape[1]
hidden_size = 128
output_size = 4
batch_size = 64
num_epochs = 20
learning_rate = 0.001
max_len = 100

train_dataset = CustomDataset(train_df, word_vectors, vocab, max_len)
test_dataset = CustomDataset(test_df, word_vectors, vocab, max_len)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

model = LSTMClassifier(input_size, hidden_size, output_size).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}")

model.eval()
correct = 0
total = 0
predicted_labels = []
true_labels = []
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        predicted_labels.extend(predicted.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

predicted_labels = np.array(predicted_labels)
true_labels = np.array(true_labels)

precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')

train_predicted_labels = np.array(train_predicted_labels)
train_true_labels = np.array(train_true_labels)
train_conf_matrix = confusion_matrix(train_true_labels, train_predicted_labels)

test_predicted_labels = np.array(test_predicted_labels)
test_true_labels = np.array(test_true_labels)
test_conf_matrix = confusion_matrix(test_true_labels, test_predicted_labels)

print(f"Accuracy on test set: {100 * correct / total}%")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print("Confusion Matrix (Training Set):\n", train_conf_matrix)
print("Confusion Matrix (Test Set):\n", test_conf_matrix)

torch.save(model.state_dict(), 'skip-gram-classification-model.pt')