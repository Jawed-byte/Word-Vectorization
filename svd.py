# -*- coding: utf-8 -*-
"""svd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cPBBiw3QVK3CNDrO29CyBotp3xn7_ZxY
"""

import pandas as pd
import numpy as np
import torch
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import svds
import string
from collections import defaultdict

data = pd.read_csv("train.csv")
descriptions = data['Description'].str.lower().tolist()[:20000]

translator = str.maketrans('', '', string.punctuation)

preprocessed_descriptions = []
for desc in descriptions:
    desc = desc.translate(translator)
    preprocessed_descriptions.append(desc.split())

vocab = defaultdict(int)
for desc in preprocessed_descriptions:
    for word in desc:
        vocab[word] += 1

word_to_idx = {word: idx for idx, (word, _) in enumerate(vocab.items())}
idx_to_word = {idx: word for word, idx in word_to_idx.items()}

co_occurrence_matrix = csr_matrix((len(vocab), len(vocab)), dtype=np.float32)
window_size = 2

for desc in preprocessed_descriptions:
    for i, word in enumerate(desc):
        word_idx = word_to_idx[word]
        for j in range(max(i - window_size, 0), min(i + window_size, len(desc))):
            if i != j:
                context_word = desc[j]
                context_word_idx = word_to_idx[context_word]
                co_occurrence_matrix[word_idx, context_word_idx] += 1

embedding_dim = 300
U, S, V = svds(co_occurrence_matrix, k=embedding_dim)

word_vectors = U @ np.diag(np.sqrt(S))

torch.save({'word_vectors': word_vectors, 'vocab': vocab}, 'svd-word-vectors.pt')
